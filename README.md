# Лабораторная работа №3: Оркестрация ETL-процессов с Apache Airflow

**Московский городской педагогический университет**

**Дисциплина:** Программные средства сбора, консолидации и аналитики данных

**Направление:** Бизнес-информатика (магистратура)

**Вариант 5**

**ФИО студента:** Давидченко Антонина Сергеевна

**Преподаватель:** Босенко Тимур Муртазович


**ПО:** 
система контейнеризации Docker и Docker Compose.
Apache Airflow (разворачивается в Docker).
база данных SQLite.
Python 3.x с библиотеками pandas, openpyxl.
email-сервис для настройки уведомлений (например, MailHog,входящий в сборку).


**Цель работы:** 

Приобретение теоретических и практических знаний в проектировании и построения автомаческих etl-процессов, используя Apache Airflow, создание конвейра данных dag, которые могут извлекать данные из csv, exel, json. Автоматически выполнять консолидацию и трансоформацию данных с помощью Python. Загружать результаты анализа в базу данных и создавать отчет , который будет отправляться на почту.


### Вариант задания №5: рассчитать общую стоимость работы (ФОТ) по каждому проекту

**Исходные данные:**
- **Файл 1 (CSV)**: Сотрудники: employee_id, position 
- **Файл 2 (Excel)**: Проекты: project_id, employee_id, hours_worked
- **Файл 3 (JSON)**: Ставки: position, rate_per_hour


**Бизнес-логика расчета:**
ФОТ рассчитывается по формуле payment = hours_worked * rate_per_hour

hours_worked — количество часов, отработанных сотрудником 
rate_per_hour — почасовая ставка сотрудника 


## Выводы по работе

После проделанной практической работы, были получены знания в написании dag файлов для построения etl-процессов с использованием Airflow. Построение автоматической консолидации, трансформации и загрузки данных в базу данных. Настройка mailhog, для автоматического получения отчета на почту  
